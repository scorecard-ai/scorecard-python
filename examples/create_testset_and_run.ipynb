{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8jauTzBvdhk"
   },
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "# For development:\n",
    "%pip install ../../scorecard-python\n",
    "# For production:\n",
    "# %pip install --pre scorecard-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xfUKyAwSwFf-"
   },
   "outputs": [],
   "source": [
    "# Fill in your API keys\n",
    "OPENAI_API_KEY = \"\"\n",
    "SCORECARD_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H6jTBlYkwsMB"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from scorecard_ai import Scorecard\n",
    "from scorecard_ai.lib import run_and_evaluate\n",
    "\n",
    "scorecard = Scorecard(api_key=SCORECARD_API_KEY)\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gYqOHL1-mYqW"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import List\n",
    "\n",
    "# Fill in your Project ID and Metric IDs\n",
    "PROJECT_ID: str = \"\"\n",
    "METRIC_IDS: List[str] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "prF7lj_smRJ6"
   },
   "outputs": [],
   "source": [
    "# The \"system under test\" -- the AI system that you want to evaluate.\n",
    "def run_system(input):\n",
    "    response = openai.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        instructions=f\"You are a tone translator that converts a user's message to a different tone ({input['tone']}). Address the recipient: {input.get('recipient')}\",\n",
    "        input=input[\"original\"],\n",
    "    )\n",
    "    return {\"rewritten\": response.output_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BLsu8l4ibmk"
   },
   "outputs": [],
   "source": [
    "# Create a Testset with a schema matching our use case\n",
    "testset = scorecard.testsets.create(\n",
    "    project_id=PROJECT_ID,\n",
    "    name=\"Tone rewriter testset\",\n",
    "    description=\"Testcases about rewriting messages in a different tone.\",\n",
    "    field_mapping={\n",
    "        # Inputs are fields that represent the input to the AI system.\n",
    "        \"inputs\": [\"original\", \"recipient\", \"tone\"],\n",
    "        # Expected fields are fields that represent the expected output of the AI system.\n",
    "        \"expected\": [\"idealRewritten\"],\n",
    "        # Metadata fields are used for grouping Testcases, but not seen by the AI system.\n",
    "        \"metadata\": [],\n",
    "    },\n",
    "    json_schema={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            # The original message.\n",
    "            \"original\": {\"type\": \"string\"},\n",
    "            # The recipient of the message.\n",
    "            \"recipient\": {\"type\": \"string\"},\n",
    "            # The tone that the message should be rewritten in.\n",
    "            \"tone\": {\"type\": \"string\"},\n",
    "            # The ideal AI-generated rewritten message.\n",
    "            \"idealRewritten\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"required\": [\"original\", \"tone\", \"idealRewritten\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "print(testset)  # noqa: T201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HqnPQWxLjoT3"
   },
   "outputs": [],
   "source": [
    "# Add Testcases matching the Testset's schema to the Testset\n",
    "testcase_response = scorecard.testcases.create(\n",
    "    testset_id=testset.id,\n",
    "    items=[\n",
    "        {\n",
    "            \"json_data\": {\n",
    "                \"original\": \"We need your feedback on the new designs ASAP.\",\n",
    "                \"tone\": \"polite\",\n",
    "                \"recipient\": \"Darius\",\n",
    "                \"idealRewritten\": \"Hi Darius, your feedback is crucial to the success of the new designs. Please share your thoughts as soon as possible.\",\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"json_data\": {\n",
    "                \"original\": \"I'll be late to the office because my cat is sleeping on my keyboard.\",\n",
    "                \"tone\": \"funny\",\n",
    "                \"recipient\": \"team\",\n",
    "                # This should return a validation error because it's missing the `idealRewritten` field.\n",
    "                \"fieldNameWithTypo\": \"Hey team! My cat's napping on my keyboard and I'm just waiting for her to give me permission to leave. I'll be a bit late!\",\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"json_data\": {\n",
    "                \"original\": \"Schedule a meeting to discuss this project.\",\n",
    "                \"tone\": \"casual\",\n",
    "                \"idealRewritten\": \"Let's find a time to chat about the project. Coffee or boba?\",\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYlr6b5DkQav"
   },
   "outputs": [],
   "source": [
    "# # Create a new Run on the Testset with the given Metrics.\n",
    "run_response = run_and_evaluate(\n",
    "    client=scorecard,\n",
    "    project_id=PROJECT_ID,\n",
    "    testset_id=testset.id,\n",
    "    metric_ids=METRIC_IDS,\n",
    "    system=lambda input, _system_config: run_system(input),\n",
    ")\n",
    "print(f\"Go to {run_response['url']} and click 'Run Scoring' to grade your Records.\")  # noqa: T201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gia_JdwunHlJ"
   },
   "outputs": [],
   "source": [
    "# Async version of the above cell\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from scorecard_ai.lib import async_run_and_evaluate\n",
    "from scorecard_ai import AsyncScorecard\n",
    "\n",
    "async_scorecard = AsyncScorecard(\n",
    "    bearer_token=SCORECARD_API_KEY\n",
    ")\n",
    "\n",
    "run_response = await async_run_and_evaluate(\n",
    "    client=async_scorecard,\n",
    "    project_id=PROJECT_ID,\n",
    "    testset_id=testset.id,\n",
    "    metric_ids=METRIC_IDS,\n",
    "    system=lambda input: run_system(input)\n",
    ")\n",
    "print(f\"Go to {run_response['url']} and click 'Run Scoring' to grade your Records.\")  # noqa: T201\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
